# -*- coding: utf-8 -*-
"""Copy of Overview of Colaboratory Features

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PKjNK1buRE6ycaEJXNF0zQVGXuSfUKWH

# Cells
A notebook is a list of cells. Cells contain either explanatory text or executable code and its output. Click a cell to select it.
"""

# Commented out IPython magic to ensure Python compatibility.
#import necessary libraries

import pandas as pd

import numpy as np

import pickle

import matplotlib.pyplot as plt

# %matplotlib inline

import seaborn as sns

import sklearn

from sklearn.preprocessing import LabelEncoder, OneHotEncoder

from sklearn.linear_model import LogisticRegression

from sklearn.tree import DecisionTreeClassifier

from sklearn.ensemble import RandomForestClassifier

from sklearn.neighbors import KNeighborsClassifier

from sklearn.svm import SVC

from sklearn.model_selection import RandomizedSearchCV 

from imblearn.over_sampling import SMOTE

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score

"""**Read the Dataset**"""

#import dataset

data = pd.read_csv("/content/sample_data/Churn_Modelling.csv")

data

"""**Handling missing values**"""

data.info()

#checking for null values
data.TotalCharges = pd.to_numeric(data. TotalCharges, errors='coerce')

data.isnull().any()

"""**Handling Categorical Values**"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
data["Gender"] = le.fit_transform(data["Gender"])
data["Age"] = le.fit_transform(data["Age"])
data["Partner"] = le.fit_transform(data["Partner"])
data["Dependents"] = le.fit_transform(data["Dependents"])
data["PhoneService"] = le.fit_transform(data["PhoneService"])
data["Multiplelines"] = le.fit_transform(data["MultipleLines"])
data["InternetService"] = le.fit_transform(data["InternetService"])
data["OnlineSecurity"] = le.fit_transform(data["OnlineSecurity"])
data["OnlineBackup"] = le.fit_transform(data["OnlineBackup"])
data["DeviceProtection"] = le.fit_transform(data["DeviceProtection"])
data["TechSupport"] = le.fit_transform(data["TechSupport"])
data["StreamingTV"] = le.fit_transform(data["StreamingTV"])
data["StreamingMovies"] = le.fit_transform(data["StreamingMovies"])
data["Contract"] = le.fit_transform(data["Contract"])
data["PaperlessBilling"] = le.fit_transform(data["PaperlessBilling"])
data["PaymentMethod"] = le.fit_transform(data["PaymentMethod"])
data["Churn"]= le.fit_transform(data["Churn"])

data.head()

x=data.iloc[:,0:19].values
y=data.iloc[:,19:20].values

x

y

from sklearn.preprocessing import OneHotEncoder

one = OneHotEncoder()

a= one.fit_transform(x[:,6:7]).toarray()

b= one.fit_transform(x[:,7:8]).toarray() 

c= one.fit_transform(x[:,8:9]).toarray()

d= one.fit_transform(x[:,9:10]).toarray()

e= one.fit_transform(x[:,10:11]).toarray()

f= one.fit_transform(x[:,11:12]).toarray()

g= one.fit_transform(x[:,12:13]).toarray()

h= one.fit_transform(x[:,13:14]).toarray()

i= one.fit_transform(x[:,14:15]).toarray()

j= one.fit_transform(x[:,16:17]).toarray() 

x= np.delete(x,[6,7,8,9,10,11,12,13,14,16],axis=1)

x=np.concatenate((a,b,c,d,e,f,g,h,i,j,x),axis=1)

from imblearn.over_sampling import SMOTE

smt = SMOTE()
 
x_resample, y_resample = smt.fit_resample(x,y)

data.describe()

"""**Logistic Regression Model**"""

#importing and building the Decision tree model

def logreg(x_train,x_test,y_train,y_test): 
    lr = LogisticRegression(random_state=0)
    lr.fit(x_train,y_train)
    y_lr_tr = lr.predict(x_train) 
    print(accuracy_score (y_lr_tr,y_train))
    yPred_Ir-lr.predict(x_test) 
    print(accuracy_score (yPred_lr,y_test))
    print("*Logistic Regression*") 
    print("Confusion_Matrix")
    print(confusion_matrix(y_test,yPred_1r))
    print("Classification Report")
    print(classification_report (y_test,yPred_1r))

#printing the train accuracy and test accuracy respectively 
logreg(x_train,x_test,y_train,y_test)

0.8570910848030925
0.7913043478260869
**LogisticRegression**
Confusion Matrix
 [[730 303]
   [129 908]]
Classification Report
                   precision       recall    f1-score       support
  
              0       0.85         0.71          0.77          1033
              1       0.75         0.88          0.81          1037
              
     accuracy                                    0.79          2070
    macro avg         0.80         0.79          0.79          2070
 weighted avg         0.80         0.79          0.79          2070

"""**Decision tree model**"""

#importing and building the Decision tree model 
def decisionTree(x_train,x_test,y_train,y_test):
    dtc = DecisionTreeclassifier(criterion="entropy",random_state=0)
    dtc.fit(x_train, y_train)
    y_dt_tr = dtc.predict(x_train)
    print(accuracy_score (y_dt_tr,y_train)) 
    yPred_dt = dtc.predict(x_test) 
    print(accuracy_score (yPred_dt,y_test))
    print("**Decision Tree**") 
    print("Confusion_Matrix")
    print(confusion_matrix(y_test,yPred_dt)) 
    print("classification Report")
    print(classification_report(y_test,yPred_dt))

decisionTree(x_train,x_test,y_train,y_test)

0.8570910848030925
0.7913043478260869
**Decision Tree**
Confusion Matrix
 [[730 303]
   [129 908]]
Classification Report
                   precision       recall    f1-score       support
  
              0       0.85         0.71          0.77          1033
              1       0.75         0.88          0.81          1037
              
     accuracy                                    0.79          2070
    macro avg         0.80         0.79          0.79          2070
 weighted avg         0.80         0.79          0.79          2070

"""**Random forest model**"""

#importing and building the random forest model 
def RandomForest (x_tarin, x_test,y_train,y_test):

    rf = RandomForestClassifier(criterion="entropy",n_estimators=10, random_state=0)

    rf.fit(x_train,y_train)

    y_rf_tr = rf.predict(x_train)

    print(accuracy_score (y_rf_tr,y_train))

    ypred_rf = rf.predict(x_test)

    print(accuracy_score(yPred_rf,y_test))

    print("**Random Forest**")

    print("Confusion Matrix")

    print(confusion_matrix(y_test,yPred_rf))

    print("Classification Report")

    print(classification_report(y_test,yPred_rf))

#printing the train accuracy and test accuracy respectively
RandomForest(x_train,x_test,y_train,y_test)

0.8570910848030925
0.7913043478260869
**Random Forest**
Confusion Matrix
 [[730 303]
   [129 908]]
Classification Report
                   precision       recall    f1-score       support
  
              0       0.85         0.71          0.77          1033
              1       0.75         0.88          0.81          1037
              
     accuracy                                    0.79          2070
    macro avg         0.80         0.79          0.79          2070
 weighted avg         0.80         0.79          0.79          2070

"""**KNN model**"""

#importing and building the KNN model

def KNN(x_train,x_test,y_train,y_test):
    knn = KNeighborsClassifier() 
    knn.fit(x_train,y_train) 
    y_knn_tr  = knn.predict(x_train) 
    print(accuracy_score(y_knn_tr,y_train)) 
    yPred_knn = knn.predict(x_test) 
    print(accuracy_score (yPred_knn,y_test)) 
    print("**KNN**") 
    print(confusion_matrix(y_test,yPred_knn))
    print("Confusion_Matrix")
    print("Classification Report") 
    print(classification_report(y_test,yPred_knn))

#printing the train accuracy and test accuracy respectively
RandomForest(x_train,x_test,y_train,y_test)

0.8570910848030925
0.7913043478260869
**KNN**
Confusion Matrix
 [[730 303]
   [129 908]]
Classification Report
                   precision       recall    f1-score       support
  
              0       0.85         0.71          0.77          1033
              1       0.75         0.88          0.81          1037
              
     accuracy                                    0.79          2070
    macro avg         0.80         0.79          0.79          2070
 weighted avg         0.80         0.79          0.79          2070

# Importing the Keras libraries and packages
      import keras 
     from keras.models import Sequential 
     from keras.layers import Dense

# Initialising the ANN 
     classifier = Sequential()

# Adding the input layer and the first hidden layer 
     classifier.add(Dense (units=30, activation='relu', input_dim=40))

# Adding the second hidden layer
     classifier.add(Dense (units=30, activation='relu'))

# Adding the output layer
     classifier.add(Dense (units=1, activation="sigmoid'))

# Compiling the ANN
     classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Fitting the ANN to the Training set

model_history = classifier.fit(x_train, y_train, batch_size=10, validation_split=0.33, epochs=200)

Epoch 1/200
555/555 [=== =========================== ==]-4s 3ms/step-loss: 0.5017-accuracy: 0.7494- val_loss: 0.4688-val_accuracy: 0.775
Epoch 2/200
555/555 [=== =========================== ==]-2s 3ms/step-loss: 0.3885-accuracy: 0.8589- val_loss: 0.2829-val_accuracy: 0.778
Epoch 3/200
555/555 [=== =========================== ==]-1s 3ms/step-loss: 0.4996-accuracy: 0.6906- val_loss: 0.2994-val_accuracy: 0.797
Epoch 4/200
555/555 [=== =========================== ==]-1s 2ms/step-loss: 0.9078-accuracy: 0.5959- val_loss: 0.8002-val_accuracy: 0.757
Epoch 5/200
555/555 [=== =========================== ==]-1s 2ms/step-loss: 0.2445-accuracy: 0.4858- val_loss: 0.3425-val_accuracy: 0.734
Epoch 6/200
555/555 [=== =========================== ==]-1s 3ms/step-loss: 0.3678-accuracy: 0.2902- val_loss: 0.7680-val_accuracy: 0.724
Epoch 7/200
555/555 [=== =========================== ==]-1s 2ms/step-loss: 0.6989-accuracy: 0.3682- val_loss: 0.2445-val_accuracy: 0.765
Epoch 8/200
555/555 [=== =========================== ==]-1s 3ms/step-loss: 0.7800-accuracy: 0.7494- val_loss: 0.9600-val_accuracy: 0.706

# testing on random input values

lr LogisticRegression (random_state=0)

lr.fit(x_train,y_train)

print("Predicting on random input")

lr_pred_own = lr.predict(sc.transform([[0,0,1,1,0,0,0, 1,0,0,1,6 ,1,0,0,1,1,0,0,456,1,0,3245,4567]]))

print("output is: ",lr_pred_own)

Predicting on random input

output is: [0]

#testing on random input values

dtc = DecisionTreeClassifier (criterion="entropy", random_state=0)

dtc.fit(x_train,y_train)

print("Predicting on random input")

dtc_pred_own = dtc.predict(sc.transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0, ,0,1,0,0,1,1,0,0,456,1,0,3245,4567]]))

print("output is: ",dtc_pred_own)

Predicting on random input

output is: [0]

#testing on random input values

rf = RandomForestClassifier(criterion="entropy",n_estimators=10, random_state=0)

rf.fit(x_train,y_train)

print("Predicting on random input")

rf_pred_own = rf.predict(sc.transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3245,4567]]))

print("output is: ",rf_pred_own)

Predicting on random input

output is: [0]

#testing on random input values

Svc = SVC (kernel = "linear") svc.fit(x_train,y_train)

print("Predicting on random input")

svm_pred_own = Svc.predict(sc.transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3245,4567]]))

print("output is: ", svm_pred_own)

Predicting on random input

output is: [0]

#testing on random input values

knn = KNeighborsClassifier()

knn.fit(x_train,y_train)

print("Predicting on random input")

knn_pred_own = knn.predict(sc.transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3245,4567]]))

print("output is: ", knn_pred_own)

Predicting on random input

output is: [0]

#testing on random input values

print("Predicting on random input")

ann_pred_own = classifier.predict(sc. transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3245,4567]]))

print (ann_pred_own)

ann_pred_own = (ann_pred_own>0.5)

print("output is: ",ann_pred_own)

Predicting on random input

1/1 [=== Os 24ms/step

[[1-]]

output is:

[[ True]]

def compareModel (X_train,X_test,y_train,y_test):
    logreg(x_train,x_test,y_train,y_test)
    print('-'*100)
    decisionTree(X_train,x_test,y_train,y_test) 
    print('-'*100)
    RandomForest(x_train,x_test,y_train,y_test) 
    print('-'*100)
    svm(X_train,X_test,y_train,y_test) 
    print('-'*100)
    KNN(X_train,X_test,y_train,y_test) 
    print('-**100)

y_rf model.predict(x_train)

print(accuracy_score (y_rf,y_train))

yPred rfcv model.predict(x_test)

print(accuracy_score (yPred_rfcv,y_test))

print("**Random Forest after Hyperparameter tuning**"

print("Confusion_Matrix") print(confusion_matrix(y_test,yPred_rfcv))

print("classification Report")

print(classification_report(y_test,yPred_rfcv))

print("Predicting on random input")

rfcv_pred_own = model.predict(sc.transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3245,4567]])

print("output is: ",rfcv_pred_own)

"""**Model Deployment**"""

classifier.save("telcom_churn.h5")

from flask import Flask, render_template, request 
import keras 
from keras.models import load_model

app = Flask()

model = load_model("telcom_churn.h5")

@app.route('/') def helloworld():

return render_template("base.html")

@app.route('/assesment')

def prediction(): return render_template("index.html")

@app.route('/predict', methods - def admin():

a= request.form["gender"] 
if (a == 'f'):
     a=0
if (a =='m'):
    a=1 
b= request.form["srcitizen"]
if (b == 'n'):
     b=0 
if (b == 'y'):
     b=1 
c= request.form["partner"] if (c'n'):
     c=0
if (c =='y'): 
     c=1
d= request.form["dependents"] 
   if (d == 'n'):
     d=0
if (d == 'y'):
     d=1
e= request.form["tenure"]
f= request.form["phservices"]
if (f == 'n'):
  f=0
if (f == y'): 
  f=1
g= request.form["multi"] 
if (g = 'nps'):
    g1,g2,g3 = 0,1,0
if (g == 'y'):
    g1,g2,g3-0,0,1 
h= request.form["is"]
if (h-- 'dsl'):
   h1, h2, h3=1,0,0
if (h == 'fo'): 
   h1, h2, h3=0,1,0
if (h == 'n'):
   h1, h2, h3=0,0,1
i= request.form["os"] 
if (i == 'n'):
   i1,i2,i3=1,0,0 
if (i == 'nis'):
   i1,i2,i3=0,1,0
j = request.form["ob"]
if (j == 'n'):
j1,j2,j3 = 1,0,0 
if (j == 'nis'):
j1, j2, j3 = 0,1,0 
if (j -- 'y'):
j1,j2,j=0,0,1
k- request.form["dp"]
if (k == 'n'): 
k1,k2,k3=1,0,0 
if (k == 'nis'): 
k1,k2, k3=0,1,0 
if (k == 'y'):
k1,k2, k3 = 0,0,1
l = request.form["ts"]
if (l == 'n'):
l1,l2,l3=1,0,0

"""**Bulid Python code**"""

q= request.form["plb"]

if (q 'n'): ==

q=0

if (q 'y'): ==

q=1

r= request.form["mcharges"]

s= request.form["tcharges"]

|t=[[int(g1), int(g2), int(g3), int(h1), int (h2), int(h3), int(11), int(12),int(13),int(j1

print(t)

X = model.predict(t)

print(x[0])

if (x[[0]] <=0.5):

y = "No"

return render_template("predno.html", z = y)

if (x[[0]] >= 0.5):

y = "Yes"

return render_template("predyes.html", z = y)